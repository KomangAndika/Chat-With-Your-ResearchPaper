{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat With Your Research Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the things i want to use: <br>\n",
    "- Built on LangChain framework.\n",
    "- LLM is Llama 3.1 from TogetherAI API.\n",
    "- Splitting the document using TextSplitter with overlap from LangChain.\n",
    "- Embed the pdf using all-mpnet-base-v2 from HuggingFace.\n",
    "- Using FAISS to store the embedding result and as the vector search too.\n",
    "- Improve the result by using query rewriter and prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some alternative or improvement: <br>\n",
    "- Other frameworks like LlamaIndex is on par with LangChain\n",
    "- You can use any other LLM provider like antrophic(claude), openai(chatgpt), perplexity, and many more.\n",
    "- Or You can host model on your own personal computer using Ollama (albeit pretty heavy workload on your PC)\n",
    "- Using semantic chunker rather than normal text splitter might yield better result (but hard on your pc too)\n",
    "- Using LLM provider for the embedding model which makes it lighter for your PC.\n",
    "- You don't have to use query rewriter or prompt engineering, this is just some simple improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/Users/komangandikawirasantosa/Chat-With-Your-ResearchPaper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# For data preprocessing and when loading into FAISS\n",
    "from glob import glob\n",
    "import getpass\n",
    "import os\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_together import TogetherEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import TogetherEmbeddings\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Chunking the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_paths = glob(\"PDF/*.pdf\")\n",
    "pages = []\n",
    "\n",
    "for path in paper_paths:\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        doc = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, \n",
    "                                      chunk_overlap=200)\n",
    "        chunked_documents = text_splitter.split_documents(doc)\n",
    "        \n",
    "        pages.extend(chunked_documents)\n",
    "    except Exception as e:\n",
    "        print('Skipping', path, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the document into the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = TogetherEmbeddings(\n",
    "    model=\"togethercomputer/m2-bert-80M-32k-retrieval\",\n",
    ")\n",
    "db = FAISS.from_documents(\n",
    "    pages,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "        base_url=\"https://api.together.xyz/v1\",\n",
    "        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "        temperature=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XML(such as <h1> </h1>) tags as a way of prompt engineering\n",
    "template = \"\"\"\n",
    "<instruction>\n",
    "You are an expert in undersatnding research paper, answer the question based on the provided context\n",
    "</instruction>\n",
    "\n",
    "Here is the context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the question:\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_template = \"\"\"\n",
    "<instruction>\n",
    "1. You are a rewriter specialist\n",
    "2. Rewrite the question for better search query by removing distraction in the question or only extracting the question\n",
    "3. Follow the output example\n",
    "4. Only output the rewrited question\n",
    "</instruction>\n",
    "\n",
    "here are the output example:\n",
    "<output_example>\n",
    "question 1: \"How tall is the Eiffel Tower? It looked so high when i was there last year\"\n",
    "answer 1: \"What is the height of the Eiffel Tower?\"\n",
    "\n",
    "question 2: \"1 oz is 28 grams, how many cm is 1 inch?\"\n",
    "answer 2: \"convert 1 inch to cm\"\n",
    "\n",
    "question 3: \"What's the main point of the article? What did the author try to convey?\"\n",
    "answer 3: \"What is the main key point of the article\"\n",
    "\n",
    "question 4: \"The Bruno Mars concert last night was dope as hell, what is the purpose EDA in data science?\"\n",
    "answer 4: \"What is the purpose EDA in data science?\"\n",
    "</output_example>\n",
    "\n",
    "Here is the question:\n",
    "<question>\n",
    "{x}\n",
    "</question>\n",
    "\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_retrieve_read_chain = (\n",
    "    {\n",
    "        \"context\": {\"x\": RunnablePassthrough()} \n",
    "                   | rewrite_prompt\n",
    "                   | llm\n",
    "                   | StrOutputParser()\n",
    "                   | retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VGG-16 model has some disadvantages, including:\n",
      "\n",
      "1. The first fully connected layer generates a great number of parameters, which increases the amount of calculation.\n",
      "2. The small and medium-sized data samples do not perform well in the deep network due to the size limits of the dataset.\n",
      "3. The limited data scale causes an overfitting problem, which results in the inability of the model to generalize.\n",
      "\n",
      "These disadvantages are mentioned in the provided context, specifically in the document \"Comparative_study_on_the_performance_of_face_recog.pdf\" on page 3.\n"
     ]
    }
   ],
   "source": [
    "# First - Normal Question\n",
    "input_query = \"What are the disadvantages of VGG-16 models?\"\n",
    "output = rewrite_retrieve_read_chain.invoke(input_query)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
