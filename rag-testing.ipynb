{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat With Your Research Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the things i want to use: <br>\n",
    "- Built on LangChain framework.\n",
    "- LLM is Llama 3.1 from TogetherAI API.\n",
    "- Splitting the document using TextSplitter with overlap from LangChain.\n",
    "- Embed the pdf using all-mpnet-base-v2 from HuggingFace.\n",
    "- Using FAISS to store the embedding result and as the vector search too.\n",
    "- Improve the result by using query rewriter and prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some alternative or improvement: <br>\n",
    "- Other frameworks like LlamaIndex is on par with LangChain\n",
    "- You can use any other LLM provider like antrophic(claude), openai(chatgpt), perplexity, and many more.\n",
    "- Or You can host model on your own personal computer using Ollama (albeit pretty heavy workload on your PC)\n",
    "- Using semantic chunker rather than normal text splitter might yield better result (but hard on your pc too)\n",
    "- Using LLM provider for the embedding model which makes it lighter for your PC.\n",
    "- You don't have to use query rewriter or prompt engineering, this is just some simple improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion into VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
